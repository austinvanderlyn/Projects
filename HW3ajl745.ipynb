{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/austinvanderlyn/Projects/blob/master/HW3ajl745.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# P1 (40pt): In the following code example explained in Lecture 6, https://colab.research.google.com/drive/13cof4XUULbUqO0s5h-cd17FskWjpyMkm, \n",
        "\n",
        "make the following changes to the neural network model sequentially in the example:\n",
        "1.   Change the number of neurons on the hidden layer to 256 units. **(10pt)**\n",
        "2.   Use the tanh activation (an activation that was popular in the early days of neural networks) instead of relu for the hidden layer. **(10pt)**\n",
        "3.   Add an additional hidden layer with 256 units and tanh activation function. **(10pt)**\n",
        "\n",
        "Retrain the newly defined model and evaluate the trained model on the testing dataset to get the accuracy. **(10pt)**  \n",
        "\n"
      ],
      "metadata": {
        "id": "e5GWh3ZrzTZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary packages\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "jEDH4K1MmD-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import data\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "OSqrvZTRmIcz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18ff25a1-80c9-4c0e-fe61-1c48a8c58a82"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNWuJmipoeI1"
      },
      "source": [
        "# P2 (60pt): Write a Python code in Colab using NumPy, Panda, Scikit-Learn and Keras to complete the following tasks:\n",
        "1.\tImport the Auto MPG dataset using pandas.read_csv(), use the attribute names as explained in the dataset description as the column names, view the strings ‘?’ as the missing value, and whitespace (i.e., ‘\\s+’) as the column delimiter. Print out the shape and first 5 rows of the DataFrame. **(5pt)**\n",
        "\n",
        "    a.\tDataset source file: http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\n",
        "\n",
        "    b.\tDataset description: http://archive.ics.uci.edu/ml/datasets/Auto+MPG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np  \n",
        "np.random.seed(100)\n",
        "\n",
        "tf.random.set_seed(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiX2FI4gZtTt"
      },
      "source": [
        "# put your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcrozGjAqYAy"
      },
      "source": [
        "2.\tDelete the “car_name” column using .drop() and drop the rows containing NULL value using .dropna(). Print out the shape of the DataFrame. **(5pt)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEJHhN65a2VV"
      },
      "source": [
        "# put your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yoyL_Epqhhg"
      },
      "source": [
        "3.\tFor the ‘origin’ column with categorical attribute, replace it with the columns with numerical attributes using one-hot encoding. Print out the shape and first 5 rows of the new DataFrame. **(5pt)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulXz4J7PAUzk"
      },
      "source": [
        "# put your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VqBe47Pquj8"
      },
      "source": [
        "4.\tSeparate the “mpg” column from other columns and view it as the label vector and others as the feature matrix. Split the data into a training set (80%) and testing set (20%) using train_test_split and print out their shapes. Print out the statistics of your training feature matrix using .describe().  **(5pt)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn-IGhUE7_1H"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# put your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0zYGXx5q_hB"
      },
      "source": [
        "5.\tNormalize the feature columns in both training and testing datasets so that their means equal to zero and variances equal to one. Note that the testing set can only be scaled by the mean and standard deviation values obtained from the training set. Describe the statistics of your normalized feature matrix of training dataset using .describe() in Pandas. **(5pt)**\n",
        "\n",
        "\n",
        "*   Option 1: You can follow the normalization steps in the code example of “Predicting house prices: a regression example” in Lecture 6.  \n",
        "\n",
        "*   Option 2: You can use StandardScaler() in Scikit-Learn as in Homework 2 but you may need to transform a NumPy array back to Pandas DataFrame using pd.DataFrame() before calling .describe(). \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlC5ooJrgjQF"
      },
      "source": [
        "# put your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqDPrWx_rDOA"
      },
      "source": [
        "6.\tBuild a sequential neural network model in Keras with two densely connected hidden layers (32 neurons and ReLU activation function for each hidden layer), and an output layer that returns a single, continuous value. Print out the model summary using .summary(). **(10pt)**\n",
        "\n",
        "*   Hint: You can follow the “Classifying movie reviews” example in Lecture 6, but need to change input_shape and last layer activation function correctly in the model definition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c26juK7ZG8j-"
      },
      "source": [
        "# put your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6P-H0ferHfJ"
      },
      "source": [
        "7.\tDefine the appropriate loss function, optimizer, and metrics for this specific problem and compile the NN model. **(10pt)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFK1JrlwPUIS"
      },
      "source": [
        "# put your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RWXSuCOrKpq"
      },
      "source": [
        "8.\tPut aside 20% of the normalized training data as the validation dataset by setting validation_split = 0.2 and set verbose = 0 to compress the model training status in Keras .fit(). Train the NN model for 100 epochs and batch size of 32 and plot the training and validation loss progress with respect to the epoch number. **(10pt)**\n",
        "\n",
        "* Remember to use GPU for training in Colab. Otherwise, you may find out of memory error or slow execution.   \n",
        "\n",
        "* There is no need to do K-fold cross-validation for this step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RYJw1bKP1R2"
      },
      "source": [
        "# put your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xLHwWMWrV1x"
      },
      "source": [
        "9.\tUse the trained NN model to make predictions on the normalized testing dataset and observe the prediction error. **(5pt)** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe7RXH3N3CWU"
      },
      "source": [
        "# put your answer here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}